{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56fc426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f73ddbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000023D5BF66710>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000023D5BF64220>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd66251",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that helps people find information.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf442a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Dhananjai, it's nice to meet you! \\n\\nAs an AI Engineer, I'm sure you're always looking for new information and ways to improve your work. How can I help you today? \\n\\nDo you have a specific question, or are you just browsing? I can help with things like:\\n\\n* **Finding research papers and articles** on specific AI topics\\n* **Explaining complex AI concepts** in a simpler way\\n* **Comparing different AI tools and frameworks**\\n* **Generating code snippets** for common AI tasks\\n\\n\\nLet me know what's on your mind! üòä \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 132, 'prompt_tokens': 35, 'total_tokens': 167, 'completion_time': 0.24, 'prompt_time': 0.00148516, 'queue_time': 0.253310128, 'total_time': 0.24148516}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--83509075-8df7-4359-af04-0aa464da6d1c-0', usage_metadata={'input_tokens': 35, 'output_tokens': 132, 'total_tokens': 167})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "chain.invoke({\"messages\": [\n",
    "    HumanMessage(content=\"Hi, my name is Dhananjai. I'm an AI Engineer\")\n",
    "]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f19b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3590d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Dhananjai! It's nice to meet you. \\n\\nBeing an AI Engineer is fascinating. What kind of projects are you working on these days?  \\n\\nI'm here to help with any information you need. Just ask! üòä  \\n\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}\n",
    "response = with_message_history.invoke([\n",
    "    HumanMessage(content=\"Hi, my name is Dhananjai. I'm an AI Engineer\")\n",
    "], config=config)\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c42104c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more complexity\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer all questions in this language: {language}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73683c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§ß‡§®‡§Ç‡§ú‡§Ø, ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π‡§æ‡§Å ‡§π‡•Ç‡§Å‡•§  \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [\n",
    "    HumanMessage(content=\"Hi, my name is Dhananjai. I'm an AI Engineer\")\n",
    "], \"language\": \"Hindi\"})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8987f62",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History Class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88c74261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¬°Hola Dhananjai! Es un placer conocerte. Soy un asistente √∫til y estoy aqu√≠ para ayudarte con lo que necesites. üòä ¬øEn qu√© te puedo ayudar?  \\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, get_session_history, input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response = with_message_history.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Hi, my name is Dhananjai. I'm an AI Engineer\")],\n",
    "    \"language\": \"Spanish\"\n",
    "}, config=config)\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba24517",
   "metadata": {},
   "source": [
    "Managing the Conversation History\n",
    "\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of messages you are passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8087629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer = trim_messages(max_tokens=70, strategy=\"last\", token_counter=model, include_system=True, allow_partial=False, start_on=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d41407",
   "metadata": {},
   "source": [
    "üîß Parameters Explained\n",
    "\n",
    "\n",
    "max_tokens=70\n",
    "Limits the total number of tokens in the trimmed message list to 70.\n",
    "\n",
    "\n",
    "strategy=\"last\"\n",
    "This strategy keeps the most recent messages and discards older ones until the token limit is met. Other strategies might include \"first\", \"middle\", etc.\n",
    "\n",
    "\n",
    "token_counter=model\n",
    "This is a function or object that knows how to count tokens (likely your language model). It‚Äôs used to calculate how many tokens each message consumes.\n",
    "\n",
    "\n",
    "include_system=True\n",
    "Ensures that system messages (like instructions or context-setting messages) are included in the trimming process.\n",
    "\n",
    "\n",
    "allow_partial=False\n",
    "If set to False, it will not include partially trimmed messages. Only full messages that fit within the token limit are kept.\n",
    "\n",
    "\n",
    "start_on=\"human\"\n",
    "Trimming will start from the last human message and work backwards. This is useful when you want to prioritize user inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9332faa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that helps people find information.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you add 2+2?', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content=\"Sure! It's 4\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"What's my favourite color?\", additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content='I like green', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Awesome! Thanks for the info.', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content=\"You're welcome! If you have any more questions, feel free to ask.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that helps people find information.\"),\n",
    "    HumanMessage(content=\"Hi, my name is Dhananjai. I'm an AI Engineer\"),\n",
    "    SystemMessage(content=\"Hello Dhananjai! How can I assist you today?\"),\n",
    "    HumanMessage(content=\"Can you add 2+2?\"),\n",
    "    SystemMessage(content=\"Sure! It's 4\"),\n",
    "    HumanMessage(content=\"What's my favourite color?\"),\n",
    "    SystemMessage(content=\"I like green\"),\n",
    "    HumanMessage(content=\"Awesome! Thanks for the info.\"),\n",
    "    SystemMessage(content=\"You're welcome! If you have any more questions, feel free to ask.\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6bde225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to personal information about you, including your name.  \\n\\nIs there anything else I can help you with? üòä  \\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer) \n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"messages\": messages + [HumanMessage(content=\"What is my name?\")],\n",
    "             \"language\": \"English\"})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "361933d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You told me your favorite color is green!  üòä \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": messages + [HumanMessage(content=\"What is my favourite color?\")],\n",
    "             \"language\": \"English\"})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b5ef325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ ‡§ï‡§ø ‡§Ü‡§™‡§ï‡§æ ‡§™‡§∏‡§Ç‡§¶‡•Ä‡§¶‡§æ ‡§∞‡§Ç‡§ó ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à‡•§  \\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's now wrap this more complicated chain in a Message History Class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, get_session_history, input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "response = with_message_history.invoke({\"messages\": [\n",
    "    HumanMessage(content=\"Hi,What is my favourite color?\")\n",
    "], \"language\": \"Hindi\"}, config=config)\n",
    "\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
